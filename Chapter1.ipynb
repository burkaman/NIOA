{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Square Root\n",
    "\n",
    "*TODO: Compare to bisection method*\n",
    "\n",
    "Simple algorithm for the square root of $k$, where $k>0$:\n",
    "\n",
    "$x_{t+1}=\\frac{1}{2}(x_t+\\frac{k}{x_t})$\n",
    "\n",
    "Derivation:\n",
    "\n",
    "$x^2=k$\n",
    "\n",
    "$x=\\frac{k}{x}$\n",
    "\n",
    "$\\frac{x}{2}=\\frac{k}{2x}=\\frac{1}{2}(\\frac{k}{x})$\n",
    "\n",
    "$\\frac{x}{2}+\\frac{x}{2}=\\frac{1}{2}(\\frac{k}{x})+\\frac{x}{2}$\n",
    "\n",
    "$x=\\frac{1}{2}(x+\\frac{k}{x})$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sqrt(k: float, x0: float, t: int) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the square root of k with t iterations\n",
    "    :param k: a positive number\n",
    "    :param x0: initial guess of the square root of k\n",
    "    :param t: number of iterations\n",
    "    :return: the estimated square root of k\n",
    "    \"\"\"\n",
    "    if t == 0:\n",
    "        return x0\n",
    "    prev = sqrt(k, x0, t - 1)\n",
    "    return 0.5*(prev + (k / prev))\n",
    "\n",
    "print(\"Estimate:\", sqrt(7, 1, 5))\n",
    "print(\"Exact:\", math.sqrt(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "How fast do different values of $x_0$ converge to the square root?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "def sqrt_iterations(k: float, prev: float, target: float, tolerance: float) -> List[float]:\n",
    "    \"\"\"\n",
    "    Estimate the square root of k until within tolerance\n",
    "    :param k: a positive number\n",
    "    :param prev: initial estimate\n",
    "    :param target: actual square root\n",
    "    :param tolerance: desired distance from target\n",
    "    :return: a list of estimates from each iteration\n",
    "    \"\"\"\n",
    "    new_value = 0.5*(prev + (k / prev))\n",
    "    if abs(new_value - target) <= tolerance:\n",
    "        return [new_value]\n",
    "    return [new_value] + sqrt_iterations(k, new_value, target, tolerance)\n",
    "\n",
    "plt.figure(\"sqrt\")\n",
    "plt.plot([1.] + sqrt_iterations(7, 1, math.sqrt(7), 0.00001))\n",
    "plt.plot([5.] + sqrt_iterations(7, 5, math.sqrt(7), 0.00001))\n",
    "plt.plot([50.] + sqrt_iterations(7, 50, math.sqrt(7), 0.00001))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Distance from target\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Newton-Raphson Method\n",
    "\n",
    "A centuries-old method for finding the zeros of a nonlinear univariate function.\n",
    "\n",
    "For a function $f(x)$, at any point $x_t$, we can approximate the value of $f(x_{t+1})$ where $\\Delta x=x_{t+1}-x_t$:\n",
    "\n",
    "$f(x_{t+1})=f(x_t+\\Delta x)\\approx f(x_t)+f^{'}(x_t)\\Delta x$\n",
    "\n",
    "To get the value at the next point, take the current value and add the slope multiplied by the distance to the next point.\n",
    "\n",
    "Solve for $x_{t+1}$:\n",
    "\n",
    "$x_{t+1}\\approx x_t+\\frac{f(x_{t+1})-f(x_t)}{f^{'}(x_t)}$\n",
    "\n",
    "To find the root, we're always hoping that $f(x_{t+1})$ will be 0. So we can approximate:\n",
    "\n",
    "$x_{t+1}=x_t-\\frac{f(x_t)}{f^{'}(x_t)}$\n",
    "\n",
    "Consider the equation $x^x=e^x, x\\in[0, \\infty)$. There are two roots, $0$ and $e$.\n",
    "\n",
    "To find them we can use $f(x)=x^x-e^x=0$.\n",
    "\n",
    "*TODO: Animate the algorithm*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sympy as sp\n",
    "\n",
    "def newton(equation: sp.Expr, derivative: sp.Expr, prev_x: float, tolerance: float) -> List[float]:\n",
    "    \"\"\"\n",
    "    Attempt to find a root of equation within tolerance\n",
    "    :param equation: a single variable equation\n",
    "    :param derivative: the derivative of equation\n",
    "    :param prev_x: initial estimate\n",
    "    :param tolerance: desired distance from 0\n",
    "    :return: a list of estimates from each iteration\n",
    "    \"\"\"\n",
    "    variables = list(equation.free_symbols)\n",
    "    if len(variables) != 1:\n",
    "        raise ValueError('Must be a single variable equation')\n",
    "    variable = variables[0]\n",
    "    new_x = prev_x - (equation.subs(variable, prev_x).evalf() / derivative.subs(variable, prev_x).evalf())\n",
    "    new_y = equation.subs(variable, new_x).evalf()\n",
    "    if not new_y.is_real:\n",
    "        return []\n",
    "    if new_y <= tolerance:\n",
    "        return [new_x]\n",
    "    return [new_x] + newton(equation, derivative, new_x, tolerance)\n",
    "\n",
    "x = sp.symbols('x')\n",
    "eq = x**x - sp.exp(x)\n",
    "\n",
    "plt.figure(\"newton1\")\n",
    "for i in range(10):\n",
    "    plt.plot([i] + newton(eq, sp.diff(eq), i, 0.00001))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Distance from target\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can extend the method to find the maximum or minimum of a function $f(\\textbf{x})$, by finding the critical points or roots of $f'(x)=0$ in a $d$-dimensional space.\n",
    "\n",
    "$x^{t+1}=x^t-\\frac{f'(x^t)}{f''(x^t)}=A(x^t)$\n",
    "\n",
    "We can also extend to work with multivariate functions by replacing $x$ with the vector $\\textbf{x}$, where\n",
    "$\\textbf{x}=(x_1, x_2, ..., x_d)^T$ is a vector of $d$ variables, and the superscript $T$ means transpose, converting the row vector to a column.\n",
    "\n",
    "*TODO: Why do we need to transpose?*\n",
    "\n",
    "The minimum/maximum method may converge slowly as $f'(x)\\rightarrow0$. Can be improved:\n",
    "\n",
    "$x^{t+1}=x^t-p\\frac{f'(x^t)}{f''(x^t)}=A(x^t, p)$\n",
    "\n",
    "$p=\\frac{1}{1-A'(x_*)}$, where $x_*$ is the optimal solution.\n",
    "\n",
    "*TODO: Not sure how to use this $p$ formula. How do we estimate $p$?*\n",
    "\n",
    "For a multivariate $f(\\textbf{x})$, consider the Taylor expansion around a known point $\\textbf{x}=\\textbf{x}_t$, and $\\Delta\\textbf{x}=\\textbf{x}-\\textbf{x}_t$.\n",
    "\n",
    "See https://www.youtube.com/watch?v=3d6DsjIBzJ4 to refresh your memory on Taylor series.\n",
    "\n",
    "$f(\\textbf{x})=f(\\textbf{x}_t)+(\\nabla f(\\textbf{x}_t))^T\\Delta \\textbf{x}+\\frac{1}{2}\\Delta \\textbf{x}^T\\nabla ^2f(\\textbf{x}_t)\\Delta \\textbf{x}+...$\n",
    "\n",
    "Here $\\nabla$ is the vector differential operator, denoting the gradient of the function.\n",
    "\n",
    "We can minimize $f(\\textbf{x})$ by solving for $\\Delta x$ in $\\nabla f(\\textbf{x}_t)+\\nabla ^2f(\\textbf{x}_t)\\Delta\\textbf{x}=0$.\n",
    "\n",
    "*TODO: Why?*\n",
    "\n",
    "Let's solve for $\\textbf{x}$:\n",
    "\n",
    "$\\nabla f(\\textbf{x}_t)+\\nabla ^2f(\\textbf{x}_t)\\Delta\\textbf{x}=0$\n",
    "\n",
    "$\\nabla f(\\textbf{x}_t)+\\nabla ^2f(\\textbf{x}_t)(\\textbf{x}-\\textbf{x}_t)=0$\n",
    "\n",
    "$\\nabla f(\\textbf{x}_t)+\\nabla ^2f(\\textbf{x}_t)\\textbf{x}-\\nabla ^2f(\\textbf{x}_t)\\textbf{x}_t=0$\n",
    "\n",
    "$\\nabla f(\\textbf{x}_t)+\\nabla ^2f(\\textbf{x}_t)\\textbf{x}=\\nabla ^2f(\\textbf{x}_t)\\textbf{x}_t$\n",
    "\n",
    "$\\frac{\\nabla f(\\textbf{x}_t)}{\\nabla ^2f(\\textbf{x}_t)}+\\textbf{x}=\\textbf{x}_t$\n",
    "\n",
    "$\\textbf{x}=\\textbf{x}_t-\\frac{\\nabla f(\\textbf{x}_t)}{\\nabla ^2f(\\textbf{x}_t)}$\n",
    "\n",
    "$\\textbf{x}=\\textbf{x}_t-H^{-1}\\nabla f(\\textbf{x}_t)$\n",
    "\n",
    "$H$ is the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix):\n",
    "\n",
    "$\\mathbf H(\\textbf{x})\\equiv \\nabla ^2f(\\textbf{x})\\equiv \\begin{bmatrix}\n",
    "   \\dfrac{\\partial^2 f}{\\partial x_1^2} & \\dfrac{\\partial^2 f}{\\partial x_1\\,\\partial x_2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_1\\,\\partial x_n} \\\\[2.2ex]\n",
    "   \\dfrac{\\partial^2 f}{\\partial x_2\\,\\partial x_1} & \\dfrac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_2\\,\\partial x_n} \\\\[2.2ex]\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\[2.2ex]\n",
    "   \\dfrac{\\partial^2 f}{\\partial x_n\\,\\partial x_1} & \\dfrac{\\partial^2 f}{\\partial x_n\\,\\partial x_2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_n^2}\n",
    " \\end{bmatrix}$\n",
    "\n",
    "Restating the min/max formula above in these terms:\n",
    "\n",
    "$\\textbf{x}^{(t+1)}=\\textbf{x}^{(t)}-H^{-1}(\\textbf{x}^{(t)})\\nabla f(\\textbf{x}^{(t)})$\n",
    "\n",
    "This is inefficient for nonquadratic functions. *TODO: Why? Try it*\n",
    "\n",
    "To speed it up, we use a smaller step size $\\alpha\\in(0, 1]$.\n",
    "\n",
    "$\\textbf{x}^{(t+1)}=\\textbf{x}^{(t)}-\\alpha H^{-1}(\\textbf{x}^{(t)})\\nabla f(\\textbf{x}^{(t)})$\n",
    "\n",
    "We can speed up further by using the identity matrix $I$ instead of calculating $H$. *TODO: Try with the Hessian too*\n",
    "\n",
    "$\\textbf{x}^{(t+1)}=\\textbf{x}^{(t)}-\\alpha I\\nabla f(\\textbf{x}^{(t)})$\n",
    "\n",
    "This is the **steepest descent method**: taking small steps down the steepest slope from the current point to find the lowest value of the objective function.\n",
    "\n",
    "Repeating our derivation of the Newton-Raphson method for multivariate functions:\n",
    "\n",
    "$\\Delta s=\\textbf{x}^{(t+1)}-\\textbf{x}^{(t)}$\n",
    "\n",
    "$f(\\textbf{x}^{(t+1)})=f(\\textbf{x}^{(t)}+\\Delta s)\\approx f(\\textbf{x}^{(t)})+(\\nabla f(\\textbf{x}^{(t)}))^T\\Delta s$\n",
    "\n",
    "We are trying to descend to the lowest value of $f(\\textbf{x})$, so we want $(\\nabla f(\\textbf{x}^{(t)}))^T\\Delta s$ to be negative.\n",
    "\n",
    "The inner product $u^Tv$ of two vectors $u$ and $v$ is most negative when they are parallel in opposite directions. *TODO: Demonstrate this*\n",
    "\n",
    "So $\\Delta s$ must be parallel opposite to $\\nabla f(\\textbf{x}^{(t)})$: $\\Delta s=-\\alpha \\nabla f(\\textbf{x}^{(t)})$\n",
    "\n",
    "$\\alpha$ is again the step size between $0$ and $1$. We want it to be large enough to make progress without overshooting.\n",
    "We can change it at each iteration for best efficiency by minimizing $f(\\textbf{x}^{(t)}-\\alpha ^{(t)}\\nabla f(\\textbf{x}^{(t)}))$, which will be a single variable optimization problem.\n",
    "\n",
    "Our final formula is now:\n",
    "\n",
    "$f(\\textbf{x}^{(t+1)})=f(\\textbf{x}^{(t)})-\\alpha^{(t)}(\\nabla f(\\textbf{x}^{(t)}))^T\\nabla f(\\textbf{x}^{(t)})$\n",
    "\n",
    "*TODO: Is alpha being called as a function here? Fix above to match what we do in code*\n",
    "\n",
    "At each step we'll first optimize $\\alpha$, using the value and gradient of $f(\\textbf{x}^{(t)})$, then solve for $\\textbf{x}^{(t+1)}$.\n",
    "\n",
    "Let's try to minimize the following:\n",
    "\n",
    "$f(x_1,x_2)=10x_1^2+5x_1x_2+10(x_2-3)^2$\n",
    "\n",
    "$(x_1,x_2)\\in [-10, 10]\\times [-15, 15]$\n",
    "\n",
    "Start at a corner: $\\textbf{x}^{(0)}=(10,15)^T$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: visualizations\n",
    "def newton_minimum_multivariate(\n",
    "        function: sp.Expr,\n",
    "        initial_value: Dict[sp.Symbol, float],\n",
    "        tolerance: float,\n",
    "        estimates: List[Dict[sp.Symbol, float]] = None,\n",
    "        max_iterations: int = 100\n",
    "    ) -> Dict[sp.Symbol, float]:\n",
    "    \"\"\"\n",
    "    Attempt to find a minimum of a multivariate function with the Newton-Raphson method\n",
    "    :param function: a multivariate function\n",
    "    :param initial_value: initial values of the free variables in function\n",
    "    :param tolerance: how close estimates should be to count as having converged\n",
    "    :param estimates: a list to populate with estimates from each iteration\n",
    "    :param max_iterations: how many iterations to try before giving up\n",
    "    :return: the estimated minimum, or None if the algorithm failed to converge within max_iterations\n",
    "    \"\"\"\n",
    "    variables = list(function.free_symbols)\n",
    "    if len(variables) != len(initial_value):\n",
    "        raise ValueError(f'Found {len(variables)} variables, but initial_value of length {len(initial_value)}')\n",
    "    if estimates is not None:\n",
    "        estimates.append(initial_value)\n",
    "    gradient = {v: sp.diff(f, v) for v in variables}\n",
    "    alpha = sp.symbols('alpha')\n",
    "    def go(prev_value: Dict[sp.Symbol, float], iterations: int):\n",
    "        if iterations >= max_iterations:\n",
    "            return None\n",
    "        gradient_prev = {v: gradient[v].evalf(subs=prev_value) for v in variables}\n",
    "        alpha_params = {v: prev_value[v] - alpha*gradient_prev[v] for v in variables}\n",
    "        alpha_optimize = function.subs(alpha_params)\n",
    "        # TODO: this should work, but alpha_value is converging to a value and then the actual minimization never converges. Investigate\n",
    "        #alpha_value = newton(alpha_optimize, alpha_optimize.diff(), 0.5, tolerance)[-1]\n",
    "        alpha_value = sp.solve(alpha_optimize.diff(), alpha)[0]\n",
    "        new_value = {v: prev_value[v] - (alpha_value * gradient_prev[v]) for v in variables}\n",
    "        if estimates is not None :\n",
    "            estimates.append(new_value)\n",
    "        if abs(sum([new_value[v] - prev_value[v] for v in variables]) / len(variables)) <= tolerance:\n",
    "            return new_value\n",
    "        return go(new_value, iterations + 1)\n",
    "    return go(initial_value, 0)\n",
    "\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "f = 10*x1**2 + 5*x1*x2 + 10*(x2 - 3)**2\n",
    "initial = {x1: 10, x2: 15}\n",
    "\n",
    "f_estimates = []\n",
    "print(newton_minimum_multivariate(f, initial, 0.0001, f_estimates))\n",
    "print(f_estimates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formalizing Optimization Problems\n",
    "\n",
    "We have a **design vector** $\\textbf{x}=(x_1, x_2, ..., x_D)^T$.\n",
    "Components $x_i$ of the $D$-dimensional vector $\\textbf{x}$ are **design** (or decision) **variables**.\n",
    "They span the **design** (or search) **space** $\\mathbb{R}^D$.\n",
    "\n",
    "We need to minimize some number of **objective** (or cost) **functions** $f_i(\\textbf{x}), (i=1, 2, ..., M), \\textbf{x}\\in\\mathbb{R}^D$.\n",
    "That is, $M$ functions of the design vector. These form the **solution** (or response) **space**.\n",
    "\n",
    "We are subject to some **constraints**: $h_j(\\textbf{x})=0, (j=1, 2, ..., J)$ and $g_k(\\textbf{x})\\leq0, (k=1, 2, ..., K)$.\n",
    "\n",
    "It is technically possible to have only constraints and no objectives. This is a **feasibility problem**, where any solution is optimal.\n",
    "\n",
    "Characteristics that make problems more difficult:\n",
    "\n",
    "- **Nonlinearity**, objective functions that don't map linearly from design vector to solution space\n",
    "- **Multimodality**, multiple local optima\n",
    "- **High dimensionality**, the size of the design space increases faster than the space an algorithm can search\n",
    "- **Constraints**, which can split feasible regions into complicated and disconnected shapes\n",
    "\n",
    "*TODO: Use Shekel and Ackley functions as worst-case tests. Also: https://www.sfu.ca/~ssurjano/optimization.html*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hill Climbing with Random Restart\n",
    "\n",
    "When searching for the maximum of a function, there may be multiple global maxima, or misleading local maxima.\n",
    "Many algorithms will find the maximum closest to the provided starting point, meaning the solution found will be somewhat random.\n",
    "\n",
    "To avoid this and make sure we search the whole design space, we can restart the algorithm many times from different randomly selected starting points.\n",
    "\n",
    "Gradient-based algorithms will still run into problems even with random restart, when dealing with complex real-world problems that don't have derivatives.\n",
    "It's also difficult to know how many random trials to do without knowing how many local peaks exist in advance.\n",
    "\n",
    "> Therefore, gradient-free methods are preferred. In fact, modern nature-inspired algorithms are almost all gradient-free optimization methods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## No-Free-Lunch Theorems\n",
    "\n",
    "> If any algorithm A outperforms another algorithm B in the search for an extremum of an objective function, then algorithm B will outperform A over other objective functions.\n",
    "\n",
    "> All algorithms for optimization will give the same average performance when averaged over *all possible* functions, which means that the universally best method does not exist for all optimization problems.\n",
    "\n",
    "> For any specific set of objective functions, some algorithms can perform much better than others.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-55871ef2",
   "language": "python",
   "display_name": "PyCharm (NIOA)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}